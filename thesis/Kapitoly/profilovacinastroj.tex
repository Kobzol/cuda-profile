Profilovací nástroj vytvářený v~této práci se skládá ze dvou částí. První, která se stará o~sběr dat za běhu programu, je popsána v~této kapitole. Druhá část, která naměřená data vizualizuje uživateli, je popsána v~kapitole \ref{sec:visualisation}.

\subsection{Specifikace požadavků}
Cílem této práce bylo vytvořit nástroj, který by umožnil zaznamenat paměťové přístupy na grafické kartě v~CUDA aplikaci. Naměřená data by měla obsahovat detailní informace o~paměťovém přístupu a~umožnit identifikovat vlákno, které přístup provedlo. Pokud budou údaje k~dispozici, měly by se také uchovávat ladící informace o~umístění instrukcí ve zdrojovém kódu a~datovém typu proměnných.
Pro účely vizualizace bylo také nutné, aby nástroj zaznamenával paměťové alokace na grafické kartě a~umožnil přiřadit je ke konkrétním přístupům do paměti.
Nástroj by měl být jednoduše použitelný a~neměl by vyžadovat provádění velkého počtu změn ve zdrojovém kódu aplikace. Jelikož nástroj není určen k~měření doby výpočtu programu a~zaznamenává velké množství dat, tak se počítá s~tím, že může znatelně ovlivnit výkon programu. Profilovaný program by ale neměl být nástrojem zpomalen natolik, aby se nedal použít alespoň pro malé testovací instance určené k~experimentování s~optimalizováním daného programu.

\subsection{Architektura}
Profilovací nástroj je tvořen LLVM průchodem a~knihovnou pro zaznamenávání paměťových přístupů za běhu aplikace. Pro implementaci obou komponent byl zvolen jazyk \emph{C++}. V~tomto jazyce je napsané LLVM i~programy, pro jejichž profilování je nástroj určený, jedná se tak o~logickou volbu pro tvorbu průchodu. C++ navíc umožňuje pracovat s~pamětí počítače na nízké úrovni, což je nutné pro získání detailních informací o~přístupech grafické karty do paměti za běhu aplikace.
Průchod nástroje se zakomponuje do běžného překladu CUDA aplikace pomocí překladače Clang. Pro použití průchodu stačí přidat několik přepínačů při překladu programu a~v~souborech s~kernely vložit hlavičkový soubor profilovacího nástroje. 
Jako profilovací metoda byla zvolena instrumentace, která narozdíl od vzorkování umožňuje zaznamenat veškeré přístupy vyskytující se ve zdrojovém kódu programu. Instrumentace sice zpomalí běh programu, nicméně měření doby výpočtu není účelem tohoto nástroje, autor práce to tedy považuje za rozumný kompromis.

Průběh překladu s~instrumentací je znázorněn na obrázku \ref{fig:instrumentation-overview}. Černě vyznačené části znázorňují standardní průběh překladu, modré části zobrazují instrumentaci a~soubory, které jsou při ní vytvářené (přerušované šipky).
Překladač Clang nejprve přeloží zdrojový kód do formátu LLVM IR. Poté průchod profilovacího nástroje přidá do IR kód, který bude za běhu zaznamenávat paměťové přístupy. Při provádění průchodu se vygenerují soubory s~metadaty, které obsahují mapování na původní zdrojový kód (čísla řádků, datové typy) pro jednotlivé kernely. Takto modifikovaný program se poté předá do backendu LLVM, který z~něj vygeneruje spustitelný soubor.

Strukturu běhu instrumentovaného programu si lze prohlédnout na obrázku \ref{fig:runtime-overview}. K~programu je přilinkována knihovna pro sběr přístupů a~knihovna pro zaznamenávání alokací. Instrukce vložené instrumentací volají funkce z~knihovny pro sběr přístupů a~zaznamenávají přístupy a~alokace. Dodatečné alokace, které nešlo instrumentovat, jsou zachyceny knihovnou pro alokace, zpracovány a~předány knihovně pro sběr přístupů.
Po každém spuštění kernelu se vygeneruje soubor s~paměťovými přístupy daného kernelu. Před ukončením programu je ještě vytvořen soubor s~údaji o~běhu celé aplikace. Všechny tyto soubory poté tvoří vstup pro vizualizační aplikaci.

\InsertFigure{instrumentation-overview}{0.7\textwidth}{Architektura instrumentace}{fig:instrumentation-overview}

\InsertFigure{runtime-overview}{0.7\textwidth}{Běh instrumentovaného programu}{fig:runtime-overview}

Profilovací nástroj se skládá z~několika izolovaných částí, které spolupracují na zaznamenávání paměťových přístupů. Tyto komponenty jsou detailně popsány v~následujících podkapitolách.

\subsection{Instrumentační průchod}
\label{sec:pass}
Instrumentační průchod se stará o~vložení volání profilovacích funkcí do instrumentovaného programu. Na svém vstupu obdrží kompilační jednotky programu (obvykle jeden implementační zdrojový soubor tvoří jednu kompilační jednotku) ve formě LLVM IR modulů. CUDA zdrojové soubory (\texttt{.cu}) jsou rozděleny na dva IR moduly. Jeden obsahuje CPU instrukce, které inicializují parametry kernelů a~spouští je. Druhý obsahuje samotný GPU kód kernelů, který je spouštěn na grafické kartě. Nástroj instrumentuje oba dva typy modulů.

\subsubsection*{Instrumentace CPU kódu}
V~modulu s~CPU kódem se vyhledají dva typy událostí -- spuštění kernelu a~alokace paměti. Před spuštěním kernelu je nutné nachystat pole přístupů na grafické kartě a~po ukončení kernelu tato data zkopírovat na procesor a~vypsat do souboru. Informace o~paměťových alokacích jsou užitečné, aby bylo možné odvodit, k~jakému typu paměti jednotlivá CUDA vlákna přistupovala.

Spuštění kernelu je ve zdrojovém kódu reprezentováno voláním funkce. Clang toto volání při překladu do IR rozdělí na volání několika CUDA funkcí, které nejprve nastaví parametry pro kernel a~poté jej spustí pomocí funkce \texttt{cudaLaunch}. Právě tato funkce je v~IR vyhledána. Těsně před ni se vloží volání funkce, která vytvoří za běhu programu kontext pro spouštěný kernel. Za ni se vloží volání funkce, která zaznamenaná data zkopíruje z~grafické karty a~dále zpracuje. Tyto funkce jsou detailněji popsány v~sekci \ref{sec:cpucollection}. Textová reprezentace instrumentovaných instrukcí v~LLVM IR je zobrazena ve výpisu \ref{code:kernelinstrumentllvm}. Výpis \ref{code:kernelinstrument} zobrazuje, jak by přibližně instrumentované spuštění kernelu mohlo vypadat ve zdrojovém kódu (řádky s~šedým pozadím značí kód vložený instrumentací).
    
\begin{lstlisting}[language=C++,
caption=Instrumentace spuštění kernelu v~LLVM IR,
label={code:kernelinstrumentllvm}]
%7 = alloca %"struct.KernelContext"
call void @_cu_initKernelContext(%"struct.KernelContext"* %7,
    [7 x i8]* @__cu_ProfileGlobal_0)
call void @__cu_kernelStart(%"struct.KernelContext"* %7)
%8 = call i32 @cudaLaunch(i8* bitcast (void (i32*)* @_Z6kernelPi to i8*))
call void @__cu_kernelEnd(%"struct.KernelContext"* %7)
call void @__cu_disposeKernelContext(%"struct.KernelContext"* %7)
\end{lstlisting}

\noindent
\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C++,
caption=Instrumentace spuštění kernelu v~C++,
label={code:kernelinstrument},
escapechar=|]
|\colorbox{insthighlight}{KernelContext ctx = \_\_cu\_initKernelContext(\textcolor[RGB]{163,21,21}{\textquotedbl kernel\textquotedbl});}|
|\colorbox{insthighlight}{\_\_cu\_kernelStart(\&ctx);}|
kernel<<<128, 256>>>(cudaPtr);
|\colorbox{insthighlight}{\_\_cu\_kernelEnd(\&ctx);}|
|\colorbox{insthighlight}{\_\_cu\_disposeKernelContext(\&ctx);}|
\end{lstlisting}
\end{minipage}

Dále se při instrumentaci vyhledají funkce pro alokaci a~dealokaci GPU paměti. Těch CUDA obsahuje několik desítek, stěžejní jsou ale dvě základní funkce \texttt{cudaMalloc} (alokace paměti) a~\texttt{cudaFree} (uvolnění paměti). Jiné funkce nástroj nevyhledává, v~případě potřeby ale lze podporu speciálních alokačních funkcí do nástroje přidat. Před alokační funkce jsou vložena volání funkcí, které za běhu programu tyto alokace zaznamenají. Jsou jim předány údaje o~velikosti alokované paměti a~ladící údaje ve formě datového typu a~umístění volání alokační funkce ve zdrojovém kódu. Po každém spuštění kernelu se vyhledají všechny paměťové alokace, které jsou v~danou chvíli aktivní a~jsou vypsány do souboru spolu s~paměťovými přístupy provedenými daným kernelem.

Při vkládání IR instrukcí, které volají funkce, je nutné jim předávat parametry. Základní datové typy, které lze uložit do registru (čísla nebo ukazatele) je předat snadné. Pro zaznamenání ladících informací je nicméně nutné předávat i~řetězce. Ty jsou v~jazyce C obvykle reprezentovány pomocí ukazatele na pole znaků ukončené znakem s~hodnotou 0. Instrumentační průchod tak musí pro předání každého řetězce nejprve vytvořit globální proměnnou, která bude obsahovat znaky daného řetězce, a~poté funkci předat ukazatel na tuto proměnnou. Pro zamezení jmenných kolizí je před název takto vytvořených proměnných přidán prefix a~na konec jejich názvu je přidáno číslo, které se po vytvoření každého řetězce inkrementuje. Proměnné jsou navíc vytvořeny se soukromou vazbou (private linkage), díky čemuž nejsou viditelné z~ostatních modulů. Aby se předešlo zbytečnému plýtvání místa, tak se při instrumentaci hodnoty řetězců uchovávají v~cache paměti. Pokud vznikne požadavek na předání řetězce, který už byl jednou vytvořen, tak se nevytváří nová globální proměnná a~pouze se předá ukazatel na již existující proměnnou.

Ukázku části instrumentačního nástroje, která instrumentuje volání funkce \texttt{cudaMalloc}, si lze prohlédnout ve výpisu \ref{code:cudamallocinstrument}. Za volání \texttt{cudaMalloc} je vloženo volání funkce \texttt{\_\_cu\_malloc}, která je naimplementovaná v~knihovně pro sběr přístupů. \texttt{cudaMalloc} přijímá jako první parametr adresu paměti, do které zapíše adresu vytvořené alokace. Tato adresa je spolu s~velikostí alokace, datového typu alokované paměti, názvu proměnné, do které se adresa ukládá a~umístění ve zdrojovém kódu předána vkládané funkci. Za běhu programu jsou pak tyto údaje zaznamenány do seznamu alokací.

Instrumentují se pouze moduly, v~jejichž zdrojovém kódu byl vložen hlavičkový soubor nástroje. Tyto moduly jsou rozpoznány podle toho, že v~nich jsou nadefinovány funkce z~tohoto hlavičkového souboru. Při instrumentaci je ale samozřejmě nutné tyto funkce vynechat, jinak by došlo k~přetečení zásobníku kvůli vloženým voláním, které by volaly rekurzivně samy sebe. Pro každou funkci se tak nejprve kontroluje, jestli její název neobsahuje prefix nástroje. Pokud ano, tak je při instrumentaci ignorována.

\begin{listing}
\begin{minted}[tabsize=4]{cpp}
void MemoryAlloc::handleCudaMalloc(CallInst* call)
{
	auto emitter = this->context.createEmitter(call->getNextNode());
	
	Value* addressLoad = emitter.getBuilder().CreateLoad(call->getOperand(0));
	
	auto& values = this->context.getValues();
	Type* valueType = getMallocValueType(call);
	std::string typeStr = this->context.getTypes().stringify(valueType);

	auto debug = getCudaMallocDebug(call);
	emitter.malloc(
		addressLoad,
		call->getOperand(1),
		values.int64(valueType->getPrimitiveSizeInBits() / 8),
		values.createGlobalCString(typeStr),
		values.createGlobalCString(debug.getName()),
		values.createGlobalCString(getFullPath(debug))
	);
}
\end{minted}
\caption{Instrumentace funkce \texttt{cudaMalloc}}
\label{code:cudamallocinstrument}
\end{listing}
 
\subsubsection*{Instrumentace GPU kódu}
V~kódu pro grafickou kartu se nejprve vyhledají všechny kernely. Ty lze detekovat pomocí atributu, který nastavuje Clang při překladu zdrojového kódu. Na začátek každého kernelu je přidán inicializační kód, který vždy provede pouze první vlákno z~výpočetní mřížky (vlákno na pozici $0.0.0.0.0.0$). Tento kód zaznamená informace o~využití sdílené paměti a~rozměrech výpočetní mřížky pro dané spuštění kernelu. Za inicializační blok je do kódu vložena synchronizační instrukce, aby ostatní vlákna počkala na konec inicializace.

Dále jsou v~kernelu nalezeny všechny přístupy do paměti (čtení i~zápis). Z~těch jsou vybrány přístupy do globální nebo sdílené paměti grafické karty. Přístupy do registrů a~k~lokálním proměnným obvykle netvoří úzká hrdla aplikací a~zbytečně by zahlcovaly vizualizaci. Instrumentování těchto přístupů lze případně volitelně zapnout. Přístupy do sdílené paměti jsou detekovány pomocí jednoduché heuristiky. Pro daný přístup se analyzuje, odkud načítá nebo kam zapisuje data (přes ukazatel, přístupem do struktury, pomocí lokální proměnné atd.). Tento postup je aplikován rekurzivně, dokud se nedojde k~bodu, který již nelze dále analyzovat (například ukazatel u~kterého není znám adresní prostor). Pokud na konci tohoto řetězce je proměnná naalokovaná ve sdílené paměti, tak je přístup označen za přístup do sdílené paměti. LLVM obsahuje vestavěnou podporu pro pokročilejší verzi této heuristiky, která analyzuje ukazatele, tato analýza však přestala fungovat pro kernely v~nových verzích LLVM.

Před zápis, resp. za načtení je vloženo zavolání funkce, která za běhu daný přístup zaznamená. Popis těchto funkcí je v~sekci \ref{sec:gpucollection}. Zápis lze instrumentovat před jeho provedením, protože je známá hodnota, která se zapisuje. Načtení je instrumentováno až poté, co se provede, aby šlo zaznamenat hodnotu, která byla načtena.

Ve výpisu \ref{code:instrumentedkernel} je znázorněno, jak by mohl vypadat instrumentovaný kernel ve zdrojovém kódu.
    
Intrumentovány jsou opět pouze ty CUDA soubory, do kterých byl vložen hlavičkový soubor nástroje \texttt{CuprRuntime.h}. Tímto lze určit, které kernely se mají profilovat na úrovni zdrojových souborů. Dále lze filtrovat profilované kernely pomocí regulárního výrazu. Parametry ovlivňující instrumentaci jsou uvedeny v~sekci \ref{sec:parameters}.

\begin{listing}
\begin{minted}[escapeinside=||]{cpp}
__global__ void kernel(int* data)
{
    __shared__ int sharedMem[10];
    if (__cu_isFirstThread()) {
        __cu_markSharedBuffer(/* adresa */ sharedMem,
            /* velikost pole */ sizeof(sharedMem),
            /* velikost prvku pole */ sizeof(sharedMem[0]),
            /* id datového typu */ 0
        );
        __cu_storeDimensions(); // uložení rozměrů mřížky
    }
    __syncthreads(); // synchronizace vláken
    
    __cu_load(/* adresa */ data + threadIdx.x, /* velikost */ sizeof(int),
        /* adresní prostor - globální paměť */ 0,
        /* id datového typu */ 0,
        /* lokace v kódu */ 0, /* hodnota */ data[threadIdx.x]
    );
    __cu_store(sharedMem + threadIdx.x, sizeof(int),
        /* adresní prostor - sdílená paměť */ 1,
        0, 1, sharedMem[threadIdx.x]
    );
    sharedMem[threadIdx.x] = data[threadIdx.x];
}
\end{minted}
\caption{Ukázka instrumentovaného kernelu}
\label{code:instrumentedkernel}
\end{listing}

Při instrumentaci se pro každý instrumentovaný kernel vytvoří soubor s~metadaty. Tento soubor obsahuje mapovací tabulku datových typů a~umístění v~kódu pro jednotlivé paměťové alokace a~paměťové přístupy. Tyto ladící údaje jsou reprezentovány řetězci, které však nelze na grafickou kartu předat tak, jak to bylo popsáno výše při popisu instrumentace CPU kódu. Jednak by řetězce zbytečně plýtvaly místem na grafické kartě, a~také by se pro každý z~nich musela speciálně alokovat paměť. Z~toho důvodu jsou na grafickou kartu datové typy a~umístění v~kódu předávány v~podobě čísel. Mapovací tabulka v~tomto souboru pak lze použít pro zpětné získání názvů datových typů a~umístění ve zdrojovém kódu. Kromě ladících metadat se do tohoto souboru ukládá také obsah zdrojového souboru s~daným kernelem. Díky tomu pak lze při vizualizaci filtrovat přístupy do paměti dle jejich umístění ve zdrojovém kódu.
    
Místo generování souboru pro každý kernel by bylo uživatelsky přívětivější, kdyby se metadata vložila přímo do instrumentovaného programu, který by je mohl do jednoho souboru zapsat až při svém spuštění. Instrumentační nástroj by mohl sesbírat veškeré informace o~programu a~do jeho vstupního bodu vygenerovat kód, který by metadata vypsal do souboru. LLVM nicméně instrumentačnímu průchodu předkládá jednotlivé moduly po jednom, zcela izolovaně a~bez jasně určeného pořadí. Mohlo by se tak stát, že modul se vstupním bodem programu přijde dříve než budou zpracovány všechny ostatní moduly a~tím pádem by nebyla k~dispozici veškerá metadata. Z~tohoto důvodu je nutné generovat soubory s~metadaty zvlášť po instrumentaci každého modulu.

Po spuštění instrumentovaného programu se v~jeho pracovním adresáři vytvoří složka, jejíž název se skládá z~prefixu $cupr$ a~UNIX timestampu s~časem spuštění programu. V~této složce se za běhu programu vytváří soubory pro jednotlivá spuštění kernelu (pro každé spuštění kernelu je vygenerován jeden soubor s~paměťovými přístupy). Zároveň se do této složky nakopírují veškerá metadata kernelů, která jsou nalezena v~pracovní složce profilovaného programu, aby poté šlo celou složku jednoduše nahrát do vizualizační aplikace.  

Výhodou této formy statické instrumentace je, že jsou k~dispozici údaje ze zdrojového kódu a~lze tak použít ladící údaje o~datových typech a~umístění instrukcí v~kódu. Tyto údaje za běhu aplikace už totiž nejsou zcela k~dispozici. Nevýhodou tohoto přístupu je, že instrumentovat lze pouze moduly, který jsou překládány, nelze tedy upravit již přeložené externí knihovny. To má za následek, že pokud by například došlo k~alokaci CUDA paměti v~externí knihovně, tak by tato alokace nebyla zaznamenána, protože by její volání nebylo instrumentované. Z~tohoto důvodu byla pro účely profilovacího nástroje vytvořena jednoduchá knihovna, která dokáže sledovat CUDA alokace za běhu aplikace. Zaznamená se tak alespoň část informací o~externích alokacích. Tato knihovna je popsaná v~sekci \ref{sec:runtimetracker}. Ke spuštění kernelu samozřejmě může také dojít v~externí knihovně. V~tom případě je ale pravděpodobné, že autorem kernelu není uživatel nástroje a~nemá tedy smysl profilovat v~něm paměťové přístupy.
    
\subsection{Sběr dat na CPU}
\label{sec:cpucollection}
Profilovací nástroj obsahuje knihovnu pro správu alokací grafické karty a~formátovaný výpis zaznamenaných přístupů. Jak bylo zmíněno v~sekci \ref{sec:pass}, před každým spuštěním kernelu je volána funkce z~této knihovny, která inicializuje kontext kernelu. Kontext obsahuje informace o~kernelu (jméno, rozměry výpočetní mřížky), naměřená data o~paměťových přístupech a~alokacích sdílené paměti a~také časovač pro měření doby výpočtu kernelu. Při jeho inicializaci se zapne časovač a~na grafické kartě se naalokuje paměť pro ukládání informací o~paměťových přístupech. Po ukončení kernelu se zaznamenaná data přenesou z~grafické karty do paměti procesoru a~poté jsou zformátovány a~vypsány do souboru. 
Synchronní kopírování dat z~GPU po každém spuštění kernelu způsobí, že v~profilovaném programu nebude možné překrývat spouštění více kernelů naráz. To teoreticky může ovlivnit chování programu, pokud tuto funkcionalitu využívá, protože serializací kernelů může dojít ke zpomalení výpočtu. Kernely jsou serializovány, aby se na grafické kartě nemuselo vytvářet více bufferů pro paměťové přístupy. Ty jsou velké a~na kartě by mohla dojít pamět. Navíc by bylo komplikované určovat, který buffer použít, kvůli omezení externích proměnných, které je popsáno v~následující sekci.

Kromě správy kontextů si knihovna také udržuje seznam všech alokací na grafické kartě. Pro danou alokaci se zaznamenává její adresa a~velikost, ladící údaje (pokud jsou k~dispozici) a~stav - po uvolnění paměti se daná alokace označí jako neplatná. Díky tomu by šlo i~detekovat přístupy do nevalidní paměti, to ale není motivací vyvíjeného nástroje. Pro tuto funkcionalitu lze použít například nástroj CUDA Memcheck\footnote{\url{https://docs.nvidia.com/cuda/cuda-memcheck/index.html}}. Údaje o~alokacích jsou knihovně předávány buď pomocí funkcí vložených instrumentací nebo pomocí knihovny pro zaznamenávání alokací popsané v~sekci \ref{sec:runtimetracker}.

Jakmile jsou přístupy zkopírovány z~GPU na CPU, tak dojde k~jejich shluknutí dle warpu. Každý paměťový přístup na grafické kartě je prováděn v~rámci jednoho warpu. Když 32 vláken ve warpu provede paměťový přístup, tak většina údajů o~daném přístupu budou těmito vlákny sdílená (typ přístupu, velikost, datový typ, lokace v~kódu atd.). Lišit se budou pouze pozice jednotlivých vláken, adresy, ke kterým přistupovaly a~hodnoty, které četly nebo zapisovaly. Z~tohoto důvodu jsou přístupy zařazeny do warpů, které společné údaje sdílejí. Skupina daného přístupu je identifikována pomocí pozice vlákna ve výpočetním mřížce, id warpu přístupu a~časového razítka. Díky tomuto shlukování instrumentované programy generují menší soubory. Zároveň je potřeba i~pro vizualizaci, je tak výhodnější ho udělat jednou přímo v~instrumentovaném programu a~ne pokaždé při načítání souborů v~prohlížeči, kde shlukování může trvat dlouho.

\subsection{Sběr dat na GPU}
\label{sec:gpucollection}
Pro účely zaznamenání paměťových přístupů je v~nástroji poskytován hlavičkový soubor s~několika GPU funkcemi. Ten musí být vložen do každého souboru s~kernely, který chce uživatel instrumentovat. To je způsobeno tím, že překladač Clang nepodporuje tzv. \emph{device linking} při CUDA kompilaci. Kvůli tomu nelze připojit k~CUDA programu externí knihovnu s~kódem pro grafickou kartu. Kód pro sběr dat na GPU tak musí být manuálně vložen do zdrojového kódu. Jedná se o~jedinou úpravu kódu, kterou je nutné pro profilování provést.
Z~důvodu nemožnosti odkazovat se na externí proměnné a~funkce musí být samotná implementace funkcí na straně CPU i~GPU přímo v~hlavičkovém souboru. To znamená, že když se tento hlavičkový soubor vloží do více zdrojových souborů v~rámci jednoho programu, tak dojde k~chybě při překladu kvůli porušení tzv. ODR (pravidla jedné definice). Toto pravidlo je definováno v~C++ standardu \cite{cppiso} a~mimo jiné říká, že každá funkce musí být definovaná v~celém programu maximálně jednou. Toto pravidlo se dá obejít použitím modifikátoru \texttt{inline}, pomocí kterého lze překladači slíbit, že všechny definice daného symbolu v~programu jsou stejné a~nevadí tak, když jich bude více. Použitá kombinace volání GPU funkcí vložených instrumentací a~absence device linking v~Clangu bohužel znemožňuje použití tohoto modifikátoru. 
Tento problém lze vyřešit třemi možnými způsoby. Prvním řešením by bylo doimplementování chybějící funkcionality do překladače Clang, to je ale mimo rámec této práce. Další možností by bylo překládat CUDA soubory manuálně do objektových souborů a~poté je k~sobě přilinkovat pomocí překladače $nvcc$, respektive jeho linkeru $nvlink$. To by ale pro uživatele nástroje znamenalo značnou komplikaci překladu programu. Poslední možností, která byla zvolena, je ignorování pravidla ODR, což lze zajistit pomocí přepínače překladače Clang (více je uvedeno v~sekci \ref{manual:usage} v~příloze). Chyby způsobené vícenásobnou definicí stejného symbolu nejsou časté a~projevily by se při překladu bez profilování. Zároveň toto řešení neznamená komplikaci překladu aplikace, protože stačí přidat jeden přepínač. Autorovi práce tak tato možnost přišla jako nejlepší kompromis.

Na grafické kartě je potřeba při běhu kernelu zaznamenat každý paměťový přístup. Kernel vykonává až tisíce vláken najednou, je tedy potřeba zápisy přístupů synchronizovat. K~tomu jsou využity atomické instrukce. Při každém zápisu vlákno provede atomicky operaci $fetch-and-add$, při které se atomicky inkrementuje hodnota proměnné a~zároveň se vrátí její předchozí hodnota. Tato operace inkrementuje index v~bufferu, do kterého se přístupy zapisují (alokace tohoto bufferu byla popsána v~sekci \ref{sec:cpucollection}). Na původní hodnotu indexu před inkrementací se v~bufferu zapíše požadovaná hodnota. Stejný způsob zápisu do pole je použit i~pro ukládání informací o~proměnných ve sdílené paměti. Pro každý přístup se zaznamenává jeho typ (čtení, zápis), souřadnice vlákna ve výpočetní mřížce, identifikační číslo warpu, adresa a~velikost přístupu, adresní prostor (lokální, globální, sdílená paměť) a~čtená či zapisovaná hodnota. Pokud byly při instrumentaci k~dispozici, tak se uloží také datový typ přístupu a~umístění instrukce ve zdrojovém kódu. Poslední zaznamenanou informací je hodnota čítače hodinového signálu procesoru grafické karty. Tato hodnota neodpovídá reálnému času, takže nelze zjistit informace o~absolutním čase přístupu. Každý multiprocesor má svůj vlastní čítač, takže ani nelze relativně porovnávat hodnoty mezi přístupy, protože jednotlivé warpy mohly mezi přístupy změnit multiprocesor, na kterém jsou vykonávány. Tato hodnota tak slouží pouze pro shlukování přístupů do skupin, které bylo popsané v~sekci \ref{sec:cpucollection}. Všechna vlákna ve warpu vždy provedou přístup se stejným časovým razítkem, díky tomu lze určit, které přístupy byly provedeny v~rámci jednoho warpu.

\subsection{Zaznamenání alokací}
\label{sec:runtimetracker}
Tato dynamická knihovna definuje signatury funkcí pro alokaci CUDA paměti a~umožňuje tak provést kód při každém zavolání těchto alokačních funkcí. Pomocí mechanismu LD\_PRELOAD\footnote{\url{http://man7.org/linux/man-pages/man8/ld.so.8.html}} lze tuto knihovnu připojit ke CUDA programu a~zachytávat pomocí ní alokace na grafické kartě. Informace o~alokacích jsou pak předány knihovně pro sběr přístupů a~následně je zavolána původní CUDA funkce, aby k~alokaci paměti skutečně došlo. Tato knihovna byla vytvořena, aby se zachytily i~alokace paměti v~externích knihovnách, které nelze instrumentovat. Knihovna ale zachytává všechny alokace, tedy i~ty instrumentované. Instrumentační průchod vkládá volání pro zaznamenání alokace až za samotné volání alokace, aby byla k~dispozici adresa naalokované paměti. Mohlo by tak dojít k~zaznamenání alokace dvakrát. Z~tohoto důvodu se při zaznamenání nejprve zkontroluje, zda na dané adrese již neexistuje aktivní alokace. Pokud ano, tak jsou údaje o~alokaci přepsány místo vytvoření nové alokace.
Informace o~alokacích na grafické kartě by se daly získat i~pomocí rozhraní CUPTI. Pro účely tohoto nástroje však stačí velmi jednoduché zachytávání alokací. Autorovi práce přišlo jednodušší dopsat tuto funkcionalitu přímo do nástroje než vyžadovat po uživatelovi instalaci externí závislosti.

\subsection{Parametry}
\label{sec:parameters}
Profilovací nástroj lze ovlivnit pomocí parametrů, které se předávají buď při překladu anebo při spouštění instrumentovaného programu pomocí proměnných prostředí. Konkrétní názvy a~výchozí hodnoty jednotlivých parametrů jsou uvedeny v~sekci \ref{manual:parameters} v~příloze.

\begin{listing}
\begin{minted}[tabsize=4]{protobuf}
message Dim3 {
	required int32 x = 1;
	required int32 y = 2;
	required int32 z = 3;
}

message MemoryAccess {
	required Dim3 threadIdx = 1;			// pozice vlákna v bloku
	required string address = 2;			// adresa přístupu
	required string value = 3;	  		// čtená/zapisovaná hodnota
}
	
message Warp {
	repeated MemoryAccess accesses = 1;	 // přístupy
	required Dim3 blockIdx = 2;			 // pozice warpu v mřížce
	required int32 warpId = 3;			  // id warpu
	required int32 debugId = 4;			 // index ladícího údaje
	required int32 size = 5;			    // počet čtených/zapisovaných bytů
	required int32 kind = 6;			    // typ přístupu (čtení/zápis)
	required int32 space = 7;			   // adresní prostor
	required int32 typeIndex = 8;		   // index datového typu
	required string timestamp = 9;		  // časové razítko
}
\end{minted}
\caption{Protobuf schéma warpu}
\label{code:protobufaccess}
\end{listing}

\begin{description}
    \item[Instrumentování lokálních přístupů] Při instrumentaci GPU kódu jsou ignorovány čtení a~zápisy lokálních proměnných a~argumentů funkce. Pokud se při překladu nastaví tento parametr, tak budou instrumentovány všechny dostupné přístupy v~kernelech.
    \item[Filtrování kernelů podle názvu] Tento parametr umožňuje nastavit při instrumentaci programu regulární výraz, který bude použit jako filtr pro kernely. Pokud název kernelu nebude rozpoznán zadaným regulárním výrazem, tak daný kernel nebude instrumentován\footnote{Stále však platí, že aby kernel byl instrumentován, tak musí být v~jeho zdrojovém souboru vložen hlavičkový soubor \texttt{CuprRuntime.h}}.
    \item[Velikost bufferu na GPU] Tento parametr ovlivňuje velikost bufferu, který je použit pro ukládání paměťových přístupů na GPU. Pokud kernel provede moc paměťových přístupů a~velikost bufferu je přesažena, program vypíše na standardní výstup upozornění. Kapacitu bufferu pak lze zvýšit, což ale samozřejmě zmenší pamět grafické karty dostupnou pro samotný výpočet. Pokud by přístupů bylo moc a~grafická karta by neměla dostatek paměti, lze nastavit použití bufferu, který bude v~operační paměti procesoru a~namapuje se pro použití z~GPU. Tím se uvolní paměť na grafické kartě, kvůli komunikaci přes sběrnici a~použití atomických operací však může dojít ke značnému zpomalení profilované aplikace.
    \item[Formát vygenerovaných souborů] Implicitně se soubory s~paměťovými přístupy vytváří v~textovém formátu JSON. Ten je jednoduše zpracovatelný ručně i~strojově a~tvoří dnes standard pro přenos dat v~mnoha odvětvích.
    Vygenerované JSON soubory nejsou odsazené z~důvodu zmenšení velikosti souboru. Volitelně je lze naformátovat pro lepší čitelnost (např. pro ladění generovaných dat). Kernely mohou generovat velké množství přístupů a~serializace v~textovém formátu je v~tomto případě neefektivní. Proto je nabízena i~alternativa ve formě binární serializace. Otestováno bylo několik knihoven pro serializaci (Protocol Buffers\footnote{\url{https://developers.google.com/protocol-buffers}}, Cap'n Proto\footnote{\url{https://capnproto.org/}} a~Flatbuffers\footnote{\url{https://google.github.io/flatbuffers/}}). Z~nich byly vybrány knihovny Protocol Buffers (Protobuf) a~Cap'n Proto. Obě knihovny umožňují serializaci dat v~pevně stanoveném binárním formátu. Díky tomu jsou soubory vygenerované těmito knihovnami několikrát menší než v~případě JSONu. Protocol Buffers byl zvolen, protože se jedná o~běžně využívaný serializační formát a~má dobrou podporu mezi programovacími jazyky. Cap'n Proto není tolik používaný, ale je několikrát rychlejší než Protobuf díky tomu, že používá stejné datové struktury pro reprezentaci dat v~paměti i~v~přenosovém formátu. Obě knihovny jsou nepovinné, profilovací nástroj tedy lze přeložit i~bez nich (v~tom případě lze použít pouze JSON). Vizualizační komponenta podporuje všechny tři formáty. Definici datové struktury warpu ve formátu Protobuf si lze prohlédnout ve výpisu \ref{code:protobufaccess}. Některé hodnoty (jako například časové razítko nebo adresa paměti) jsou z~důvodu kompatibility s~JSONem uloženy jako řetězec, JSON (potažmo ani Javascript) totiž nepodporuje 64-bitová celá čísla.
    \item[Komprese dat] Pokud jsou vygenerované soubory moc velké, lze použít kompresi s~kódováním \emph{gzip}. Ke kompresi je využita knihovna zlib\footnote{\url{https://www.zlib.net}}, která je opět nepovinná pro překlad nástroje. Kompresi lze libovolně kombinovat se všemi výstupními formáty.
    \item[Paralelizace formátování] Formátování a~serializace velkého počtu paměťových přístupů je náročná, proto je tato činnost v~knihovně pro sběr dat na CPU paralelizována. Vnitřně se vytváří $thread pool$ (skupina vláken), která zpracovává žádosti o~formátování přístupů. Pokud by instrumentovaná aplikace sama využívala více vláken a~paralelní formátování by ji zpomalovalo nebo by paralelní zpracování vyžadovalo velké množství paměti, tak lze tuto funkcionalitu vypnout.
\end{description}
